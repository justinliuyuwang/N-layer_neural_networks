{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justin Wang December 2020\n",
    "\n",
    "This script will perform logistic regression (forward and backprop using vectorization).\n",
    "\n",
    "First, we establish the correct project (and associated info) and import the related datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import PIL\n",
    "from matplotlib import pyplot\n",
    "import random\n",
    "\n",
    "#project = \"RashData\"\n",
    "#positive = \"Lyme_Positive\"\n",
    "#negative = \"Lyme_Negative\"\n",
    "\n",
    "#project = \"chest_xray\"\n",
    "#positive = \"NORMAL\"\n",
    "#negative = \"PNEUMONIA\"\n",
    "\n",
    "#project = \"chest_covid\"\n",
    "#positive = \"NORMAL\"\n",
    "#negative = [\"COVID19\",\"PNEUMONIA\"]\n",
    "#need a code for splitting the negative dataset\n",
    "\n",
    "project = \"cat_dog\"\n",
    "positive = \"cats\"\n",
    "negative = \"dogs\"\n",
    "\n",
    "#import datasets    \n",
    "f = h5py.File(project+'.hdf5', \"r\")\n",
    "\n",
    "train_pos_dset = f['train/'+positive]\n",
    "train_neg_dset = f['train/'+negative]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we establish some basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_info(train_pos_dset, train_neg_dset):\n",
    "    #number of training examples for each class\n",
    "    num_train_neg = train_pos_dset.shape[0]\n",
    "    num_train_pos = train_neg_dset.shape[0]\n",
    "\n",
    "    num_features = train_pos_dset.shape[1]\n",
    "\n",
    "    #list of indices for accessing images from the datasets\n",
    "    train_neg_index_list = list(range(num_train_neg))\n",
    "    train_pos_index_list = list(range(num_train_pos))\n",
    "    \n",
    "    return num_train_neg, num_train_pos, num_features, train_neg_index_list, train_pos_index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the meat of the code, in which we iterate through epochs and iterate through mini-batches within each epoch. Forward and back propagation (a step of gradient descent) is completed once for each mini-batch. Minibatches are shuffled and newly generated for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layers, num_features):\n",
    "    #layers is a list describing the number of nodes in each layer, including the output layer but not the features (i.e layers=[5,2,3,1] has 5 nodes in layer 1, 2 nodes in layer 2.... 1 node in the output layer)\n",
    "    \n",
    "    W = []\n",
    "    B = []    \n",
    "    #W and B are lists containing weights and biases matrices - the i'th item in W is the i'th set of weights (i.e the second item in W is the matrix of weights between the first and second hidden layers)\n",
    "    \n",
    "    x = 0\n",
    "    while x < len(layers):\n",
    "        if x == 0:\n",
    "            W.append(np.random.rand(layers[x],num_features))\n",
    "        else:\n",
    "            W.append(np.random.rand(layers[x],layers[x-1]))\n",
    "            \n",
    "        B.append(np.zeroes(layers[x],1))\n",
    "        x+=1\n",
    "            \n",
    "    parameters = {\"W\":W,\n",
    "                  \"B\":B}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_creation(train_neg_index_list,train_pos_index_list,mini_batch_size)\n",
    "    #list of indices for a given mini-batch (used to access the images for this mini-batch from the dataset)\n",
    "    pos_mini_batch_index_list = []\n",
    "    neg_mini_batch_index_list = []\n",
    "\n",
    "        \n",
    "    num_instances_batched=0\n",
    "\n",
    "    #iterate through and add all the mini batches for this epoch to the collector lists pos_mini_batch_index_list & neg_mini_batch_index_list\n",
    "    #(only making sure we get all the positive instances.. we don't need to iterate through ALL negative examples if theres a mismatch between # pos and # neg)\n",
    "    while num_instances_batched < num_train_pos:\n",
    "\n",
    "\n",
    "        #slice locations on shuffled index list (indicates which shuffled indices are associated with this current mini-batch)\n",
    "        #this slice should cover half a batch's worth of indices since we're splitting the classes 50-50\n",
    "        slice_start_neg = int(num_instances_batched%num_train_neg)\n",
    "        slice_end_neg = int((num_instances_batched+mini_batch_size/2)%num_train_neg)\n",
    "\n",
    "\n",
    "        #NOTE: we grab from the first few indices (wraparound) if we have to loop around/duplicate instances to complete batches... this is same as grabbing randoms to fill in because the list is randomized\n",
    "        #if we don't have a wrap-around\n",
    "        if slice_end_neg > slice_start_neg:\n",
    "            neg_mini_batch_index_list.append(train_neg_index_list[slice_start_neg:slice_end_neg]) \n",
    "        #if we have a wrap-around\n",
    "        else:\n",
    "            neg_mini_batch_index_list.append(train_neg_index_list[slice_start_neg:] + train_neg_index_list[:slice_end_neg]) \n",
    "\n",
    "\n",
    "\n",
    "        #repeat for positive class\n",
    "        slice_start_pos = int(num_instances_batched%num_train_pos)\n",
    "        slice_end_pos = int((num_instances_batched+mini_batch_size/2)%num_train_pos)\n",
    "\n",
    "\n",
    "        if slice_end_pos > slice_start_pos:\n",
    "            pos_mini_batch_index_list.append(train_pos_index_list[slice_start_pos:slice_end_pos])\n",
    "        else:\n",
    "            pos_mini_batch_index_list.append(train_pos_index_list[slice_start_pos:] + train_pos_index_list[:slice_end_pos]) \n",
    "\n",
    "\n",
    "\n",
    "        #iterate to next mini-batch\n",
    "        num_instances_batched+=mini_batch_size/2\n",
    "        \n",
    "        \n",
    "            \n",
    "    return neg_mini_batch_index_list, pos_mini_batch_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_label_creation(neg_mini_batch_index, pos_mini_batch_index, train_pos_dset, train_neg_dset):\n",
    "    return \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, features, labels, num_features):\n",
    "    \n",
    "    for x in:\n",
    "        \n",
    "    #matrices containing our features and their labels\n",
    "            \n",
    "    \n",
    "        #(m,n[0])\n",
    "        features = np.empty([mini_batch_size,num_features])    \n",
    "\n",
    "        #(1,m)\n",
    "        labels = np.empty([mini_batch_size,1])                       \n",
    "\n",
    "\n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        #fancy indexing? super slow - 10-20x slower because sorting of indices is needed\n",
    "        #extract the dataset images pointed to by the current minibatch, combining both negative and positive classes\n",
    "        for index in neg_mini_batch_index_list:\n",
    "            features[counter] = train_neg_dset[index]\n",
    "\n",
    "            labels[counter] = 0\n",
    "            counter+=1\n",
    "\n",
    "\n",
    "        for index in pos_mini_batch_index_list:\n",
    "            features[counter] = train_pos_dset[index]\n",
    "\n",
    "            labels[counter] = 1\n",
    "            counter+=1  \n",
    "            \n",
    "            \n",
    "        ####\n",
    "        # SHALLOW NEURAL NETWORK - SPECIFIC CODE STARTS HERE\n",
    "        ####\n",
    "        #\n",
    "        # \n",
    "        # FORWARD AND BACKPROP CODE HERE\n",
    "        # Z = WT X + b\n",
    "\n",
    "        Z = np.empty([1,mini_batch_size])\n",
    "        Z = W.transpose\n",
    "        # A = sigmoid(Z)\n",
    "        # dZ = A-Y\n",
    "        # dw = 1/m XdZT\n",
    "        # db = 1/m np.sum(dZ)\n",
    "        # w:= w- alphadw\n",
    "        # b:= b - alphadb\n",
    "        #\n",
    "        #\n",
    "        ####\n",
    "        # SHALLOW NEURAL NETWORK - SPECIFIC CODE ENDS HERE\n",
    "        ####\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(features)\n",
    "        print(features.shape)\n",
    "        print(labels)\n",
    "        print(labels.shape)\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def model(train_pos_dset, train_neg_dset, val_pos_dset, val_neg_dset, mini_batch_size =32, layers, epochs = 1000, print_cost=False):\n",
    "\n",
    "    num_train_neg, num_train_pos, num_features, train_neg_index_list, train_pos_index_list = training_info(train_pos_dset, train_neg_dset)\n",
    "    \n",
    "    parameters = intialize(layers, num_features)\n",
    "    \n",
    "    #iterate through all the epochs\n",
    "    while epoch>0:\n",
    "\n",
    "        #shuffle the lists of indices, effectively shuffling the training instances among the mini-batches \n",
    "        random.shuffle(train_neg_index_list)\n",
    "        random.shuffle(train_pos_index_list)\n",
    "\n",
    "        neg_mini_batch_index_list, pos_mini_batch_index_list = mini_batch_creation(train_neg_index_list,train_pos_index_list,mini_batch_size)\n",
    "        \n",
    "        #iterate through all the mini-batches\n",
    "        for neg_mini_batch, pos_mini_batch in zip(neg_mini_batch_index_list, pos_mini_batch_index_list):\n",
    "            features, labels = feature_label_creation(neg_mini_batch, pos_mini_batch)\n",
    "            parameters = gradient_descent(parameters, features, labels, num_features)\n",
    "        \n",
    "        #iterate to next epoch\n",
    "        epoch-=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
