{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justin Wang December 2020\n",
    "\n",
    "This script will perform logistic regression (forward and backprop using vectorization).\n",
    "\n",
    "First, we establish the correct project (and associated info) and import the related datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import PIL\n",
    "from matplotlib import pyplot\n",
    "import random\n",
    "import math\n",
    "\n",
    "#project = \"RashData\"\n",
    "#positive = \"Lyme_Positive\"\n",
    "#negative = \"Lyme_Negative\"\n",
    "\n",
    "#project = \"chest_xray\"\n",
    "#positive = \"NORMAL\"\n",
    "#negative = \"PNEUMONIA\"\n",
    "\n",
    "#project = \"chest_covid\"\n",
    "#positive = \"NORMAL\"\n",
    "#negative = [\"COVID19\",\"PNEUMONIA\"]\n",
    "#need a code for splitting the negative dataset\n",
    "\n",
    "project = \"cat_dog\"\n",
    "positive = \"cats\"\n",
    "negative = \"dogs\"\n",
    "\n",
    "#import datasets    \n",
    "f = h5py.File(project+'.hdf5', \"r\")\n",
    "\n",
    "train_pos_dset = f['train/'+positive]\n",
    "train_neg_dset = f['train/'+negative]\n",
    "\n",
    "val_pos_dset = f['val/'+positive]\n",
    "val_neg_dset = f['val/'+negative]\n",
    "\n",
    "#test_pos_dset = f['test/'+positive]\n",
    "#test_neg_dset = f['test/'+negative]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we establish some basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_info(train_pos_dset, train_neg_dset):\n",
    "    #number of training examples for each class\n",
    "    num_train_neg = train_pos_dset.shape[0]\n",
    "    num_train_pos = train_neg_dset.shape[0]\n",
    "\n",
    "    num_features = train_pos_dset.shape[1]\n",
    "\n",
    "    #list of indices for accessing images from the datasets\n",
    "    train_neg_index_list = list(range(num_train_neg))\n",
    "    train_pos_index_list = list(range(num_train_pos))\n",
    "    \n",
    "    return num_train_neg, num_train_pos, num_features, train_neg_index_list, train_pos_index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the meat of the code, in which we iterate through epochs and iterate through mini-batches within each epoch. Forward and back propagation (a step of gradient descent) is completed once for each mini-batch. Minibatches are shuffled and newly generated for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layers, num_features):\n",
    "    #layers is a list describing the number of nodes in each layer, including the output layer but not the features (i.e layers=[5,2,3,1] has 5 nodes in layer 1, 2 nodes in layer 2.... 1 node in the output layer)\n",
    "    \n",
    "    W = []\n",
    "    B = []    \n",
    "    #W and B are lists containing weights and biases matrices - the i'th item in W is the i'th set of weights (i.e the second item in W is the matrix of weights between the first and second hidden layers)\n",
    "    \n",
    "    x = 0\n",
    "    \n",
    "    #TODO: check if these initializations are appropriate\n",
    "    while x < len(layers):\n",
    "        if x == 0:\n",
    "            W.append(np.random.rand(layers[x],num_features))\n",
    "        else:\n",
    "            W.append(np.random.rand(layers[x],layers[x-1]))\n",
    "            \n",
    "        B.append(np.zeroes(layers[x],1))\n",
    "        x+=1\n",
    "            \n",
    "    parameters = {\"W\":W,\n",
    "                  \"B\":B}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(train_neg_index_list,train_pos_index_list,mini_batch_size)\n",
    "    #list of indices for a given mini-batch (used to access the images for this mini-batch from the dataset)\n",
    "    pos_mini_batches = []\n",
    "    neg_mini_batches = []\n",
    "\n",
    "        \n",
    "    num_instances_batched=0\n",
    "\n",
    "    #iterate through and add all the mini batches for this epoch to the collector lists pos_mini_batches & neg_mini_batches\n",
    "    #(only making sure we get all the positive instances.. we don't need to iterate through ALL negative examples if theres a mismatch between # pos and # neg)\n",
    "    while num_instances_batched < num_train_pos:\n",
    "\n",
    "\n",
    "        #slice locations on shuffled index list (indicates which shuffled indices are associated with this current mini-batch)\n",
    "        #this slice should cover half a batch's worth of indices since we're splitting the classes 50-50\n",
    "        slice_start_neg = int(num_instances_batched%num_train_neg)\n",
    "        slice_end_neg = int((num_instances_batched+mini_batch_size/2)%num_train_neg)\n",
    "\n",
    "\n",
    "        #NOTE: we grab from the first few indices (wraparound) if we have to loop around/duplicate instances to complete batches... this is same as grabbing randoms to fill in because the list is randomized\n",
    "        #if we don't have a wrap-around\n",
    "        if slice_end_neg > slice_start_neg:\n",
    "            neg_mini_batches.append(train_neg_index_list[slice_start_neg:slice_end_neg]) \n",
    "        #if we have a wrap-around\n",
    "        else:\n",
    "            neg_mini_batches.append(train_neg_index_list[slice_start_neg:] + train_neg_index_list[:slice_end_neg]) \n",
    "\n",
    "\n",
    "\n",
    "        #repeat for positive class\n",
    "        slice_start_pos = int(num_instances_batched%num_train_pos)\n",
    "        slice_end_pos = int((num_instances_batched+mini_batch_size/2)%num_train_pos)\n",
    "\n",
    "\n",
    "        if slice_end_pos > slice_start_pos:\n",
    "            pos_mini_batches.append(train_pos_index_list[slice_start_pos:slice_end_pos])\n",
    "        else:\n",
    "            pos_mini_batches.append(train_pos_index_list[slice_start_pos:] + train_pos_index_list[:slice_end_pos]) \n",
    "\n",
    "\n",
    "\n",
    "        #iterate to next mini-batch\n",
    "        num_instances_batched+=mini_batch_size/2\n",
    "        \n",
    "        \n",
    "            \n",
    "    return neg_mini_batches, pos_mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_label(neg_indices, pos_indices, train_neg_dset, train_pos_dset, mini_batch_size, num_features):\n",
    "    #matrices containing our features and their labels for training set mini-batches\n",
    "            \n",
    "    \n",
    "    #(n[0],m) ... will transpose upon return\n",
    "    features = np.empty([mini_batch_size,num_features])    \n",
    "\n",
    "    #(1,m) ... will transpose upon return\n",
    "    labels = np.empty([mini_batch_size,1])                       \n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    #fancy indexing? super slow - 10-20x slower because sorting of indices is needed\n",
    "    #extract the dataset images pointed to by the current minibatch, combining both negative and positive classes\n",
    "    for index in neg_indices:\n",
    "        features[counter] = train_neg_dset[index]\n",
    "\n",
    "        labels[counter] = 0\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "    for index in pos_indices:\n",
    "        features[counter] = train_pos_dset[index]\n",
    "\n",
    "        labels[counter] = 1\n",
    "        counter+=1  \n",
    "            \n",
    "    if counter not == mini_batch_size:\n",
    "        print(\"error: batch size - mini batch indices mismatch\")\n",
    "    \n",
    "    return features.transpose(), labels.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def leaky_relu(Z):\n",
    "    return np.where(Z > 0, Z, 0.01*Z)\n",
    "    \n",
    "\n",
    "def leaky_relu_derivative(Z):\n",
    "    return np.where(Z > 0, 1.0, 0.05)\n",
    "\n",
    "    \n",
    "    \n",
    "def sigmoid(Z):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-Z)) * (1 - (1 / (1 + np.exp(-Z))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(parameters, features, layers):\n",
    "    \n",
    "    Z = []\n",
    "    A = []\n",
    "    G = []\n",
    "    for layer in range(len(layers)):\n",
    "        \n",
    "        A_last_layer = np.zeroes([1,1])\n",
    "        \n",
    "        if layer==0:\n",
    "            A_last_layer = features\n",
    "        else:\n",
    "            A_last_layer = A[layer-1]\n",
    "        \n",
    "        \n",
    "        W_current_layer = parameters['W'][layer]\n",
    "        B_current_layer = parameters['B'][layer]\n",
    "        \n",
    "        \n",
    "        Z_current_layer = np.dot(W_current_layer, A_last_layer) + B_current_layer\n",
    "        \n",
    "        \n",
    "        #if output layer\n",
    "        if layer == len(layers)-1:\n",
    "            G_current_layer = \"sigmoid\"\n",
    "            A_current_layer = sigmoid(Z_current_layer)\n",
    "        else:\n",
    "            G_current_layer = \"leaky_relu\"\n",
    "            A_current_layer = leaky_relu(Z_current_layer)\n",
    "        \n",
    "        \n",
    "        Z.append(Z_current_layer)\n",
    "        A.append(A_current_layer)\n",
    "        G.append(G_current_layer)\n",
    "            \n",
    "    return Z, A, G\n",
    "    \n",
    "#batchnorm in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(parameters, features, labels, layers, Z, A, G, mini_batch_size):\n",
    "    dZ = []\n",
    "    dW= []\n",
    "    db = []\n",
    "    for layer in reversed(range(len(layers))):\n",
    "        \n",
    "        \n",
    "        A_last_layer = np.zeroes([1,1])\n",
    "        \n",
    "        \n",
    "        if layer == 0:\n",
    "            A_last_layer = features\n",
    "        else:\n",
    "            A_last_layer = A[layer-1]\n",
    "        \n",
    "        \n",
    "        if layer == len(layers)-1:\n",
    "            dZ_current_layer = A[layer] - labels\n",
    "            dW_current_layer = (1/mini_batch_size)*np.dot(dZ_current_layer, A[layer-1].transpose())\n",
    "            db_current_layer = (1/mini_batch_size)*np.sum(dZ_current_layer, axis=1, keepdims=True)\n",
    "        \n",
    "        else:\n",
    "            #TODO implement variable for G that can change depending on layer\n",
    "            #for now, we know all the hidden layers use leaky relu so we'll directly use that derivative\n",
    "            dZ_current_layer = np.dot(parameters[W][layer+1].transpose(), dZ[len(layers)-1-layer-1]) * leaky_relu_derivative(Z[layer])\n",
    "            dW_current_layer = (1/mini_batch_size)*np.dot(dZ_current_layer, A_last_layer.transpose())\n",
    "            db_current_layer = (1/mini_batch_size)*np.sum(dZ_current_layer, axis=1, keepdims=True)\n",
    "        \n",
    "        #these lists are going to be backwards (layer X ... layer 2, layer 1)\n",
    "        dZ.append(dZ_current_layer)\n",
    "        dW.append(dW_current_layer)\n",
    "        db.append(db_current_layer)\n",
    "    \n",
    "    dW.reverse()\n",
    "    db.reverse()\n",
    "    updates = {'dW':dW,\n",
    "                'db':db}\n",
    "    return updates\n",
    "\n",
    "#TODO: variable g(Z) setting/matrix   \n",
    "#may just use leaky relu\n",
    "\n",
    "#batchnorm in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, labels, mini_batch_size):\n",
    "     \n",
    "    cost = -1/mini_batch_size*np.sum(np.multiply(np.log(A[-1]),labels)+np.multiply(np.log(1-A[-1]),1-labels))\n",
    "               \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, features, labels, num_features, learning_rate, layers, mini_batch_size):\n",
    "        \n",
    "    Z, A, G = forward_prop(parameters, features, layers)\n",
    "    cost = compute_cost(A,labels, mini_batch_size)\n",
    "    updates = back_prop(paramaters, features, labels, layers, Z, A, G, mini_batch_size)\n",
    "    \n",
    "    #update the parameters \n",
    "    for layer in range(len(parameter[W])):\n",
    "        parameter[W][layer] = parameter[W][layer] - learning_rate * updates[dW][layer]\n",
    "        parameter[B][layer] = parameter[B][layer] - learning_rate * updates[db][layer]\n",
    "    \n",
    "    #code here if we want to track or print cost after every mini-batch\n",
    "    \n",
    "    return parameters, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def model(train_pos_dset, train_neg_dset, mini_batch_size =32, layers = [1], epochs = 10000, print_cost=False, learning_rate=0.005):\n",
    "    \n",
    "    #lets us track cost and later graph it\n",
    "    cost_tracker = np.zeroes([1000])\n",
    "    cost = 0\n",
    "    \n",
    "    num_train_neg, num_train_pos, num_features, train_neg_index_list, train_pos_index_list = training_info(train_pos_dset, train_neg_dset)\n",
    "    \n",
    "    parameters = intialize(layers, num_features)\n",
    "    \n",
    "    #iterate through all the epochs\n",
    "    while epoch>0:\n",
    "\n",
    "        #shuffle the lists of indices, effectively shuffling the training instances among the mini-batches \n",
    "        random.shuffle(train_neg_index_list)\n",
    "        random.shuffle(train_pos_index_list)\n",
    "        \n",
    "        #neg_mini_batches and pos_mini_batches are lists of lists of indices pointing to training instances\n",
    "        neg_mini_batches, pos_mini_batches = mini_batch(train_neg_index_list,train_pos_index_list,mini_batch_size)\n",
    "        \n",
    "        #iterate through all the mini-batches (e.g. neg_indices contains the indices of a single mini-batch of negative class training instances)\n",
    "        for neg_indices, pos_indices in zip(neg_mini_batches, pos_mini_batches):\n",
    "            \n",
    "            #extract feature list and label list from this mini-batch\n",
    "            features, labels = feature_label(neg_indices, pos_indices,train_neg_dset, train_pos_dset, mini_batch_size, num_features)\n",
    "            parameters, cost = gradient_descent(parameters, features, labels, num_features, learning_rate, layers, mini_batch_size)\n",
    "        \n",
    "            #maybe also write code for printing cost after each mini batch\n",
    "             #maybe write code for % success at test\n",
    "                \n",
    "        \n",
    "        #TODO write code for printing cost after each epoch\n",
    "        #maybe write code for % success at test\n",
    "        \n",
    "        #TODO track cost\n",
    "        cost_tracker[10000-epoch] = cost\n",
    "        \n",
    "        #iterate to next epoch\n",
    "        epoch-=1\n",
    "    \n",
    "    #TODO print final cost or costs AND plot graph of costs x iterations\n",
    "    \n",
    "    print(\"final cost: \"+cost)\n",
    "    plt.plot(cost_tracker)\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "    #TODO write code for % success at test, cost using prediction()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prediction(pos_dset, neg_dset, parameters, layers):\n",
    "    predictions_on_pos = np.empty([pos_dset.shape[0],1])\n",
    "    predictions_on_neg = np.empty([pos_dset.shape[1],1])\n",
    "    \n",
    "    for pos in range(pos_dset.shape[0]):\n",
    "        predictions_on_pos[pos] = forward_prop(parameters, pos_dset[pos].transpose(), layers)\n",
    "    \n",
    "    for neg in range(neg_dset.shape[0]):\n",
    "        predictions_on_neg[neg] = forward_prop(parameters, neg_dset[neg].transpose(), layers)\n",
    "    \n",
    "    \n",
    "    return predictions_on_pos, predictions_on_neg\n",
    "    \n",
    "    \n",
    "    \n",
    "def accuracy(predictions, labels, show_wrong=3):\n",
    "    #TODO show accuracy and images that are incorrectly classified - default 3 from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parameters = model(train_pos_dset, train_neg_dset, mini_batch_size =32, layers = [2,5,5,1], epochs = 10000, print_cost=False, learning_rate=0.005)\n",
    "predictions_on_pos, predictions_on_neg = prediction(train_pos_dset, train_neg_dset, parameters, layers = [2,5,5,1])\n",
    "predictions_on_pos, predictions_on_neg = prediction(val_pos_dset, val_neg_dset, parameters, layers = [2,5,5,1])\n",
    "predictions_on_pos, predictions_on_neg = prediction(test_pos_dset, test_neg_dset, parameters, layers = [2,5,5,1])\n",
    "accuracy()\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
