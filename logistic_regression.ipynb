{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justin Wang December 2020\n",
    "\n",
    "This script will perform logistic regression (forward and backprop using vectorization).\n",
    "\n",
    "First, we establish the correct project (and associated info) and import the related datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import PIL\n",
    "from matplotlib import pyplot\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "#project = \"RashData\"\n",
    "#positive = \"Lyme_Positive\"\n",
    "#negative = \"Lyme_Negative\"\n",
    "\n",
    "#project = \"chest_xray\"\n",
    "#positive = \"NORMAL\"\n",
    "#negative = \"PNEUMONIA\"\n",
    "\n",
    "#project = \"chest_covid\"\n",
    "#positive = \"NORMAL\"\n",
    "#negative = [\"COVID19\",\"PNEUMONIA\"]\n",
    "#need a code for splitting the negative dataset\n",
    "\n",
    "project = \"cat_dog\"\n",
    "positive = \"cats\"\n",
    "negative = \"dogs\"\n",
    "\n",
    "#import datasets    \n",
    "f = h5py.File(project+'.hdf5', \"r\")\n",
    "\n",
    "train_pos_dset = f['train/'+positive]\n",
    "train_neg_dset = f['train/'+negative]\n",
    "\n",
    "val_pos_dset = f['val/'+positive]\n",
    "val_neg_dset = f['val/'+negative]\n",
    "\n",
    "#test_pos_dset = f['test/'+positive]\n",
    "#test_neg_dset = f['test/'+negative]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we establish some basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_info(train_pos_dset, train_neg_dset):\n",
    "    #number of training examples for each class\n",
    "    num_train_neg = train_pos_dset.shape[0]\n",
    "    num_train_pos = train_neg_dset.shape[0]\n",
    "\n",
    "    num_features = train_pos_dset.shape[1]\n",
    "\n",
    "    #list of indices for accessing images from the datasets\n",
    "    train_neg_index_list = list(range(num_train_neg))\n",
    "    train_pos_index_list = list(range(num_train_pos))\n",
    "    \n",
    "    return num_train_neg, num_train_pos, num_features, train_neg_index_list, train_pos_index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the meat of the code, in which we iterate through epochs and iterate through mini-batches within each epoch. Forward and back propagation (a step of gradient descent) is completed once for each mini-batch. Minibatches are shuffled and newly generated for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layers, num_features):\n",
    "    #layers is a list describing the number of nodes in each layer, including the output layer but not the features (i.e layers=[5,2,3,1] has 5 nodes in layer 1, 2 nodes in layer 2.... 1 node in the output layer)\n",
    "    \n",
    "    W = []\n",
    "    B = []    \n",
    "    #W and B are lists containing weights and biases matrices - the i'th item in W is the i'th set of weights (i.e the second item in W is the matrix of weights between the first and second hidden layers)\n",
    "    \n",
    "    x = 0\n",
    "    \n",
    "    #TODO: check if these initializations are appropriate\n",
    "    while x < len(layers):\n",
    "        if x == 0:\n",
    "            W.append(np.random.rand(layers[x],num_features)*0.01)\n",
    "        else:\n",
    "            W.append(np.random.rand(layers[x],layers[x-1])*0.01)\n",
    "            \n",
    "        B.append(np.zeros((layers[x],1)))\n",
    "        x += 1\n",
    "            \n",
    "    parameters = {\"W\":W,\n",
    "                  \"B\":B}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(train_neg_index_list, train_pos_index_list, mini_batch_size, num_train_pos, num_train_neg):\n",
    "    #list of indices for a given mini-batch (used to access the images for this mini-batch from the dataset)\n",
    "    pos_mini_batches = []\n",
    "    neg_mini_batches = []\n",
    "\n",
    "        \n",
    "    num_instances_batched=0\n",
    "\n",
    "    #iterate through and add all the mini batches for this epoch to the collector lists pos_mini_batches & neg_mini_batches\n",
    "    #(only making sure we get all the positive instances.. we don't need to iterate through ALL negative examples if theres a mismatch between # pos and # neg)\n",
    "    while num_instances_batched < num_train_pos:\n",
    "\n",
    "\n",
    "        #slice locations on shuffled index list (indicates which shuffled indices are associated with this current mini-batch)\n",
    "        #this slice should cover half a batch's worth of indices since we're splitting the classes 50-50\n",
    "        slice_start_neg = int(num_instances_batched%num_train_neg)\n",
    "        slice_end_neg = int((num_instances_batched+mini_batch_size/2)%num_train_neg)\n",
    "\n",
    "\n",
    "        #NOTE: we grab from the first few indices (wraparound) if we have to loop around/duplicate instances to complete batches... this is same as grabbing randoms to fill in because the list is randomized\n",
    "        #if we don't have a wrap-around\n",
    "        if slice_end_neg > slice_start_neg:\n",
    "            neg_mini_batches.append(train_neg_index_list[slice_start_neg:slice_end_neg]) \n",
    "        #if we have a wrap-around\n",
    "        else:\n",
    "            neg_mini_batches.append(train_neg_index_list[slice_start_neg:] + train_neg_index_list[:slice_end_neg]) \n",
    "\n",
    "\n",
    "\n",
    "        #repeat for positive class\n",
    "        slice_start_pos = int(num_instances_batched%num_train_pos)\n",
    "        slice_end_pos = int((num_instances_batched+mini_batch_size/2)%num_train_pos)\n",
    "\n",
    "\n",
    "        if slice_end_pos > slice_start_pos:\n",
    "            pos_mini_batches.append(train_pos_index_list[slice_start_pos:slice_end_pos])\n",
    "        else:\n",
    "            pos_mini_batches.append(train_pos_index_list[slice_start_pos:] + train_pos_index_list[:slice_end_pos]) \n",
    "\n",
    "\n",
    "\n",
    "        #iterate to next mini-batch\n",
    "        num_instances_batched+=mini_batch_size/2\n",
    "        \n",
    "        \n",
    "            \n",
    "    return neg_mini_batches, pos_mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_label(neg_indices, pos_indices, train_neg_dset, train_pos_dset, mini_batch_size, num_features):\n",
    "    #matrices containing our features and their labels for training set mini-batches\n",
    "            \n",
    "    \n",
    "    #(n[0],m) ... will transpose upon return\n",
    "    features = np.empty([mini_batch_size,num_features])    \n",
    "\n",
    "    #(1,m) ... will transpose upon return\n",
    "    labels = np.empty([mini_batch_size,1])                       \n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    #fancy indexing? super slow - 10-20x slower because sorting of indices is needed\n",
    "    #extract the dataset images pointed to by the current minibatch, combining both negative and positive classes\n",
    "    for index in neg_indices:\n",
    "        features[counter] = train_neg_dset[index]\n",
    "\n",
    "        labels[counter] = 0\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "    for index in pos_indices:\n",
    "        features[counter] = train_pos_dset[index]\n",
    "\n",
    "        labels[counter] = 1\n",
    "        counter+=1  \n",
    "            \n",
    "    if counter != mini_batch_size:\n",
    "        print(\"error: batch size - mini batch indices mismatch\")\n",
    "    \n",
    "    return features.transpose(), labels.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def leaky_relu(Z):\n",
    "    return np.where(Z > 0, Z, 0.01*Z)\n",
    "    \n",
    "\n",
    "def leaky_relu_derivative(Z):\n",
    "    return np.where(Z > 0, 1.0, 0.05)\n",
    "\n",
    "    \n",
    "    \n",
    "def sigmoid(Z):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-Z)) * (1 - (1 / (1 + np.exp(-Z))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(parameters, features, layers):\n",
    "    \n",
    "    Z = []\n",
    "    A = []\n",
    "    G = []\n",
    "    for layer in range(len(layers)):\n",
    "        \n",
    "        A_last_layer = np.zeros((1,1))\n",
    "        \n",
    "        if layer==0:\n",
    "            A_last_layer = features\n",
    "        else:\n",
    "            A_last_layer = A[layer-1]\n",
    "        \n",
    "        \n",
    "        W_current_layer = parameters['W'][layer]\n",
    "        B_current_layer = parameters['B'][layer]\n",
    "        \n",
    "        \n",
    "        Z_current_layer = np.dot(W_current_layer, A_last_layer) + B_current_layer\n",
    "        \n",
    "        \n",
    "        #if output layer\n",
    "        if layer == len(layers)-1:\n",
    "            G_current_layer = \"sigmoid\"\n",
    "            A_current_layer = sigmoid(Z_current_layer)\n",
    "        else:\n",
    "            G_current_layer = \"leaky_relu\"\n",
    "            A_current_layer = leaky_relu(Z_current_layer)\n",
    "        \n",
    "        \n",
    "        Z.append(Z_current_layer)\n",
    "        A.append(A_current_layer)\n",
    "        G.append(G_current_layer)\n",
    "            \n",
    "    return Z, A, G\n",
    "    \n",
    "#batchnorm in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(parameters, features, labels, layers, Z, A, G, mini_batch_size):\n",
    "    dZ = []\n",
    "    dW= []\n",
    "    db = []\n",
    "    for layer in reversed(range(len(layers))):\n",
    "        \n",
    "        \n",
    "        A_last_layer = np.zeros((1,1))\n",
    "        \n",
    "        \n",
    "        if layer == 0:\n",
    "            A_last_layer = features\n",
    "        else:\n",
    "            A_last_layer = A[layer-1]\n",
    "        \n",
    "        \n",
    "        if layer == len(layers)-1:\n",
    "            dZ_current_layer = A[layer] - labels\n",
    "            dW_current_layer = (1/mini_batch_size)*np.dot(dZ_current_layer, A[layer-1].transpose())\n",
    "            db_current_layer = (1/mini_batch_size)*np.sum(dZ_current_layer, axis=1, keepdims=True)\n",
    "        \n",
    "        else:\n",
    "            #TODO implement variable for G that can change depending on layer\n",
    "            #for now, we know all the hidden layers use leaky relu so we'll directly use that derivative\n",
    "            dZ_current_layer = np.dot(parameters['W'][layer+1].transpose(), dZ[len(layers)-1-layer-1]) * leaky_relu_derivative(Z[layer])\n",
    "            dW_current_layer = (1/mini_batch_size)*np.dot(dZ_current_layer, A_last_layer.transpose())\n",
    "            db_current_layer = (1/mini_batch_size)*np.sum(dZ_current_layer, axis=1, keepdims=True)\n",
    "        \n",
    "        #these lists are going to be backwards (layer X ... layer 2, layer 1)\n",
    "        dZ.append(dZ_current_layer)\n",
    "        dW.append(dW_current_layer)\n",
    "        db.append(db_current_layer)\n",
    "    \n",
    "    dW.reverse()\n",
    "    db.reverse()\n",
    "    updates = {'dW':dW,\n",
    "                'db':db}\n",
    "    return updates\n",
    "\n",
    "#TODO: variable g(Z) setting/matrix   \n",
    "#may just use leaky relu\n",
    "\n",
    "#batchnorm in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, labels, mini_batch_size):\n",
    "   \n",
    "    cost = (-1/mini_batch_size)*np.sum(np.multiply(np.log(A[-1]),labels) + np.multiply(np.log(1-A[-1]),1-labels))\n",
    "    #print(type(cost))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, features, labels, num_features, learning_rate, layers, mini_batch_size):\n",
    "        \n",
    "    Z, A, G = forward_prop(parameters, features, layers)\n",
    "    cost = compute_cost(A,labels, mini_batch_size)\n",
    "    updates = back_prop(parameters, features, labels, layers, Z, A, G, mini_batch_size)\n",
    "    \n",
    "    #update the parameters \n",
    "    for layer in range(len(parameters['W'])):\n",
    "        parameters['W'][layer] = parameters['W'][layer] - learning_rate * updates['dW'][layer]\n",
    "        parameters['B'][layer] = parameters['B'][layer] - learning_rate * updates['db'][layer]\n",
    "    \n",
    "    #code here if we want to track or print cost after every mini-batch\n",
    "    \n",
    "    return parameters, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def model(train_pos_dset, train_neg_dset, mini_batch_size =32, layers = [1], epochs = 5000, learning_rate=0.02):\n",
    "    \n",
    "    lasttime = time.time()\n",
    "    \n",
    "    #lets us track cost and later graph it\n",
    "    cost_tracker = np.zeros((epochs))\n",
    "    cost = 0\n",
    "    initial_epochs = epochs\n",
    "    num_train_neg, num_train_pos, num_features, train_neg_index_list, train_pos_index_list = training_info(train_pos_dset, train_neg_dset)\n",
    "    \n",
    "    parameters = initialize(layers, num_features)\n",
    "    \n",
    "    #iterate through all the epochs\n",
    "    while epochs>0:\n",
    "\n",
    "        #shuffle the lists of indices, effectively shuffling the training instances among the mini-batches \n",
    "        random.shuffle(train_neg_index_list)\n",
    "        random.shuffle(train_pos_index_list)\n",
    "        \n",
    "        #neg_mini_batches and pos_mini_batches are lists of lists of indices pointing to training instances\n",
    "        neg_mini_batches, pos_mini_batches = mini_batch(train_neg_index_list,train_pos_index_list,mini_batch_size, num_train_pos, num_train_neg)\n",
    "        \n",
    "        #iterate through all the mini-batches (e.g. neg_indices contains the indices of a single mini-batch of negative class training instances)\n",
    "        for neg_indices, pos_indices in zip(neg_mini_batches, pos_mini_batches):\n",
    "            \n",
    "            #extract feature list and label list from this mini-batch\n",
    "            features, labels = feature_label(neg_indices, pos_indices,train_neg_dset, train_pos_dset, mini_batch_size, num_features)\n",
    "            parameters, cost = gradient_descent(parameters, features, labels, num_features, learning_rate, layers, mini_batch_size)\n",
    "        \n",
    "            #maybe also write code for printing cost after each mini batch\n",
    "             #maybe write code for % success at test\n",
    "                \n",
    "        \n",
    "        #TODO write code for printing cost after each epoch\n",
    "        #maybe write code for % success at test\n",
    "        \n",
    "        #TODO track cost\n",
    "        cost_tracker[initial_epochs-epochs] = cost\n",
    "        \n",
    "        if epochs % 10 == 0:\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"epoch \"+ str(initial_epochs-epochs) +\": \"+ str(cost))\n",
    "            \n",
    "            if epochs != initial_epochs:\n",
    "                print(\"time since last 10 epochs: \"+ str(time.time() - lasttime))\n",
    "                lasttime = time.time()\n",
    "            \n",
    "        #iterate to next epoch\n",
    "        epochs-=1\n",
    "        \n",
    "    #TODO print final cost or costs AND plot graph of costs x iterations\n",
    "    \n",
    "    print(\"final cost: \"+cost)\n",
    "    plt.plot(cost_tracker)\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "    #TODO write code for % success at test, cost using prediction()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prediction(pos_dset, neg_dset, parameters, layers):\n",
    "    predictions_on_pos = np.empty([pos_dset.shape[0],1])\n",
    "    predictions_on_neg = np.empty([neg_dset.shape[0],1])\n",
    "    \n",
    "    for pos in range(pos_dset.shape[0]):\n",
    "        Z, A, G = forward_prop(parameters, pos_dset[pos].transpose(), layers)\n",
    "        predictions_on_pos[pos] = round(A[-1][0][0])\n",
    "        #might not need indices here\n",
    "        \n",
    "    for neg in range(neg_dset.shape[0]):\n",
    "        Z, A, G = forward_prop(parameters, neg_dset[neg].transpose(), layers)\n",
    "        predictions_on_neg[neg] = round(A[-1][0][0])\n",
    "    \n",
    "    return predictions_on_pos, predictions_on_neg\n",
    "    \n",
    "    \n",
    "    \n",
    "def accuracy(predictions_on_pos, predictions_on_neg, show_wrong=3):\n",
    "    \n",
    "    pos_labels = np.ones((predictions_on_pos.shape[0],1))\n",
    "    neg_labels = np.zeros((predictions_on_neg.shape[0],1))\n",
    "    accuracy = float((np.dot(pos_labels,predictions_on_pos) + np.dot(1-neg_labels,1-predictions_on_neg))/float(predictions_on_neg.shape[0]+predictions_on_pos.shape[0])*100)\n",
    "    \n",
    "    false_pos = pos_labels-predictions_on_pos\n",
    "    false_neg = neg_labels-predictions_on_neg\n",
    "    \n",
    "    false_pos_indices = []\n",
    "    false_neg_indices = []\n",
    "    \n",
    "    counter= 0\n",
    "    x = 0\n",
    "    while x < false_pos.shape[0] and counter< show_wrong:\n",
    "        if false_pos[x] != 0:\n",
    "            false_pos_indices.append(x)\n",
    "            counter+=1\n",
    "        \n",
    "        x+=1\n",
    "        \n",
    "        \n",
    "    counter= 0\n",
    "    x = 0\n",
    "    while x < false_neg.shape[0] and counter< show_wrong:\n",
    "        if false_neg[x] != 0:\n",
    "            false_neg_indices.append(x)\n",
    "            counter+=1\n",
    "        \n",
    "        x+=1\n",
    "    \n",
    "    \n",
    "    return accuracy, false_pos_indices, false_neg_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 0.6931458199886162\n",
      "epoch 10: 0.6931472482266557\n",
      "time since last 10 epochs: 54.67650008201599\n",
      "epoch 20: 0.6935941960173637\n",
      "time since last 10 epochs: 38.85163712501526\n",
      "epoch 30: 0.6933724877805225\n",
      "time since last 10 epochs: 37.270437240600586\n",
      "epoch 40: 0.6928707207496675\n",
      "time since last 10 epochs: 39.21857213973999\n",
      "epoch 50: 0.6813674136824968\n",
      "time since last 10 epochs: 33.79556083679199\n",
      "epoch 60: 0.6970368480455046\n",
      "time since last 10 epochs: 38.99338698387146\n",
      "epoch 70: 0.693411725900465\n",
      "time since last 10 epochs: 41.38505005836487\n",
      "epoch 80: 0.6944514758089451\n",
      "time since last 10 epochs: 47.6208336353302\n",
      "epoch 90: 0.6807914989289023\n",
      "time since last 10 epochs: 55.897335052490234\n",
      "epoch 100: 0.6580631842142032\n",
      "time since last 10 epochs: 43.85175609588623\n",
      "epoch 110: 0.6336669731238068\n",
      "time since last 10 epochs: 35.37406778335571\n",
      "epoch 120: 0.6330349911456371\n",
      "time since last 10 epochs: 34.017138957977295\n",
      "epoch 130: 0.590750939911991\n",
      "time since last 10 epochs: 33.9655396938324\n",
      "epoch 140: 0.6668251680241297\n",
      "time since last 10 epochs: 34.0970458984375\n",
      "epoch 150: 0.6103299579532038\n",
      "time since last 10 epochs: 33.91667413711548\n",
      "epoch 160: 0.6605666579684348\n",
      "time since last 10 epochs: 51.0328528881073\n",
      "epoch 170: 0.5563016083324921\n",
      "time since last 10 epochs: 36.68266320228577\n",
      "epoch 180: 0.7450037748167404\n",
      "time since last 10 epochs: 38.097315073013306\n",
      "epoch 190: 0.4678383889160699\n",
      "time since last 10 epochs: 41.613476037979126\n",
      "epoch 200: 0.6705024203169136\n",
      "time since last 10 epochs: 40.34131407737732\n",
      "epoch 210: 0.5356493667953438\n",
      "time since last 10 epochs: 39.760053873062134\n",
      "epoch 220: 0.5987586612434801\n",
      "time since last 10 epochs: 38.53713798522949\n",
      "epoch 230: 0.5313266494851598\n",
      "time since last 10 epochs: 39.76040291786194\n",
      "epoch 240: 0.6598663010638159\n",
      "time since last 10 epochs: 39.54863882064819\n",
      "epoch 250: 0.5038703701746794\n",
      "time since last 10 epochs: 42.96550917625427\n",
      "epoch 260: 0.5388405810879343\n",
      "time since last 10 epochs: 61.62234902381897\n",
      "epoch 270: 0.5465388877859184\n",
      "time since last 10 epochs: 41.61601734161377\n",
      "epoch 280: 0.5001288019550638\n",
      "time since last 10 epochs: 33.363555908203125\n",
      "epoch 290: 0.47228633330759023\n",
      "time since last 10 epochs: 38.059414863586426\n",
      "epoch 300: 0.44659477379096185\n",
      "time since last 10 epochs: 34.696966886520386\n",
      "epoch 310: 0.7348536395397366\n",
      "time since last 10 epochs: 58.635090827941895\n",
      "epoch 320: 0.5588624351959609\n",
      "time since last 10 epochs: 79.15653586387634\n",
      "epoch 330: 0.5409379453378701\n",
      "time since last 10 epochs: 37.80791783332825\n",
      "epoch 340: 0.5351745174311827\n",
      "time since last 10 epochs: 39.08377385139465\n",
      "epoch 350: 0.7114409809305005\n",
      "time since last 10 epochs: 36.952325105667114\n",
      "epoch 360: 0.7339978594049932\n",
      "time since last 10 epochs: 36.326170921325684\n",
      "epoch 370: 0.40224436976001493\n",
      "time since last 10 epochs: 61.588778018951416\n",
      "epoch 380: 0.6948301643948083\n",
      "time since last 10 epochs: 63.84649896621704\n",
      "epoch 390: 0.6931399794845972\n",
      "time since last 10 epochs: 64.15544009208679\n",
      "epoch 400: 0.6996824850823555\n",
      "time since last 10 epochs: 50.94489097595215\n",
      "epoch 410: 0.7181674842163683\n",
      "time since last 10 epochs: 47.47023820877075\n",
      "epoch 420: 0.6664995758451746\n",
      "time since last 10 epochs: 59.76293420791626\n",
      "epoch 430: 0.5085009321969554\n",
      "time since last 10 epochs: 45.473042011260986\n",
      "epoch 440: 0.369938619454629\n",
      "time since last 10 epochs: 59.3516628742218\n",
      "epoch 450: 0.6507386658682492\n",
      "time since last 10 epochs: 40.45172905921936\n",
      "epoch 460: 0.6931044758993912\n",
      "time since last 10 epochs: 37.403732776641846\n",
      "epoch 470: 0.5571590150283802\n",
      "time since last 10 epochs: 48.819870948791504\n",
      "epoch 480: 0.47335693669133017\n",
      "time since last 10 epochs: 50.06549406051636\n",
      "epoch 490: 0.6936189698678038\n",
      "time since last 10 epochs: 41.601869106292725\n",
      "epoch 500: 0.5684353053194252\n",
      "time since last 10 epochs: 38.41215395927429\n",
      "epoch 510: 0.7051465455877268\n",
      "time since last 10 epochs: 40.44503688812256\n",
      "epoch 520: 0.6252031085084886\n",
      "time since last 10 epochs: 40.87422204017639\n",
      "epoch 530: 0.38047713686306356\n",
      "time since last 10 epochs: 50.626758098602295\n",
      "epoch 540: 0.7154497608616353\n",
      "time since last 10 epochs: 56.61515212059021\n",
      "epoch 550: 0.6931670498585814\n",
      "time since last 10 epochs: 59.55748915672302\n",
      "epoch 560: 0.3813963451967764\n",
      "time since last 10 epochs: 45.71379804611206\n",
      "epoch 570: 0.6937190937110089\n",
      "time since last 10 epochs: 44.329867124557495\n",
      "epoch 580: 0.3784820188612277\n",
      "time since last 10 epochs: 42.842153787612915\n",
      "epoch 590: 0.6931159605738028\n",
      "time since last 10 epochs: 38.494773864746094\n",
      "epoch 600: 0.7346066841133114\n",
      "time since last 10 epochs: 41.22655177116394\n",
      "epoch 610: 0.7269263373042094\n",
      "time since last 10 epochs: 41.13996195793152\n",
      "epoch 620: 0.7150563960429484\n",
      "time since last 10 epochs: 37.46249866485596\n",
      "epoch 630: 0.6929986976865963\n",
      "time since last 10 epochs: 38.646002769470215\n",
      "epoch 640: 0.6504354625880828\n",
      "time since last 10 epochs: 39.5613009929657\n",
      "epoch 650: 0.4536168898643142\n",
      "time since last 10 epochs: 41.45772385597229\n",
      "epoch 660: 0.389793632295969\n",
      "time since last 10 epochs: 38.39371085166931\n",
      "epoch 670: 0.4622580411468372\n",
      "time since last 10 epochs: 42.16529083251953\n",
      "epoch 680: 0.6397253034502608\n",
      "time since last 10 epochs: 42.52264904975891\n",
      "epoch 690: 0.6931182317749236\n",
      "time since last 10 epochs: 41.79324221611023\n",
      "epoch 700: 0.4035962122607638\n",
      "time since last 10 epochs: 41.30960202217102\n",
      "epoch 710: 0.694706596576914\n",
      "time since last 10 epochs: 68.75662088394165\n",
      "epoch 720: 0.3375085061137755\n",
      "time since last 10 epochs: 42.47308683395386\n",
      "epoch 730: 0.5101462450722833\n",
      "time since last 10 epochs: 35.336162090301514\n",
      "epoch 740: 0.6371082720231002\n",
      "time since last 10 epochs: 39.33773183822632\n",
      "epoch 750: 0.7108775910576565\n",
      "time since last 10 epochs: 39.51268696784973\n",
      "epoch 760: 0.6946567334871991\n",
      "time since last 10 epochs: 38.89673089981079\n",
      "epoch 770: 0.48529181816047584\n",
      "time since last 10 epochs: 43.147301197052\n",
      "epoch 780: 0.7285945922828705\n",
      "time since last 10 epochs: 53.66330814361572\n",
      "epoch 790: 0.7057667692923377\n",
      "time since last 10 epochs: 38.74950981140137\n",
      "epoch 800: 0.39313903681413565\n",
      "time since last 10 epochs: 37.199037075042725\n",
      "epoch 810: 0.45679989152114914\n",
      "time since last 10 epochs: 38.784472942352295\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e904f07bc881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pos_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_neg_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_predictions_on_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predictions_on_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pos_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_neg_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_predictions_on_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_predictions_on_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_pos_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_neg_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#test_predictions_on_pos, test_predictions_on_neg = prediction(test_pos_dset, test_neg_dset, parameters, layers = [2,5,5,1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8e3bbe1d645b>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(train_pos_dset, train_neg_dset, mini_batch_size, layers, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#extract feature list and label list from this mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_neg_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pos_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m#maybe also write code for printing cost after each mini batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = model(train_pos_dset, train_neg_dset, mini_batch_size =32, layers = [25,10,5,1], epochs = 5000, learning_rate=0.02)\n",
    "\n",
    "train_predictions_on_pos, train_predictions_on_neg = prediction(train_pos_dset, train_neg_dset, parameters, layers = [2,5,5,1])\n",
    "val_predictions_on_pos, val_predictions_on_neg = prediction(val_pos_dset, val_neg_dset, parameters, layers = [2,5,5,1])\n",
    "#test_predictions_on_pos, test_predictions_on_neg = prediction(test_pos_dset, test_neg_dset, parameters, layers = [2,5,5,1])\n",
    "\n",
    "train_accuracy, train_false_pos_indices, train_false_neg_indices = accuracy(train_predictions_on_pos, train_predictions_on_neg, show_wrong=3)\n",
    "val_accuracy, val_false_pos_indices, val_false_neg_indices = accuracy(val_predictions_on_pos, val_predictions_on_neg, show_wrong=3)\n",
    "\n",
    "print(\"Training Accuracy: \"+train_accuracy)\n",
    "print(\"Validation Accuracy: \"+val_accuracy)\n",
    "\n",
    "\n",
    "for index_set in [train_false_pos_indices, train_false_neg_indices, val_false_pos_indices, val_false_neg_indices]:\n",
    "    if index_set == train_false_pos_indices:\n",
    "        print(\"False Positives from Training Set: \")\n",
    "    elif index_set == train_false_neg_indices:\n",
    "        print(\"False Negatives from Training Set: \")\n",
    "    elif index_set == val_false_pos_indices:\n",
    "        print(\"False Positives from Validation Set: \")\n",
    "    elif index_set == val_false_neg_indices:\n",
    "        print(\"False Negatives from Validation Set: \")\n",
    "    else:\n",
    "        print(\"error printing false prediction images\")\n",
    "        \n",
    "    for index in index_set:\n",
    "        orig_image = np.uint8((np.round(mydset[index].reshape(128,128,3) * 255.0)).astype(int))\n",
    "        pyplot.imshow(orig_image)\n",
    "\n",
    "\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
